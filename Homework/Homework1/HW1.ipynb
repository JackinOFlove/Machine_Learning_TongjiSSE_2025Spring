{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: y = -1.0000x + 4.0000, Training data: X=[2 3], y=[2 1]\n",
      "Prediction: X=0, Predicted value=4.0000, Actual value=2\n",
      "--------------------------------------------------\n",
      "Model 2: y = -0.3333x + 2.0000, Training data: X=[0 3], y=[2 1]\n",
      "Prediction: X=2, Predicted value=1.3333, Actual value=2\n",
      "--------------------------------------------------\n",
      "Model 3: y = 0.0000x + 2.0000, Training data: X=[0 2], y=[2 2]\n",
      "Prediction: X=3, Predicted value=2.0000, Actual value=1\n",
      "--------------------------------------------------\n",
      "Mean Squared Error (MSE) of leave-one-out cross-validation: 1.815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([0, 2, 3]).reshape(-1, 1)  \n",
    "y = np.array([2, 2, 1])\n",
    "\n",
    "# Define leave-one-out cross-validation function\n",
    "def leave_one_out_cv(X, y):\n",
    "    n = len(X)\n",
    "    predictions = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Training set: all samples except the i-th sample\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        y_train = np.delete(y, i)\n",
    "        \n",
    "        # Test set: the i-th sample\n",
    "        X_test = X[i].reshape(1, -1)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict the i-th sample and store the result\n",
    "        predictions[i] = model.predict(X_test)[0]\n",
    "        \n",
    "        w = model.coef_[0]\n",
    "        b = model.intercept_\n",
    "        print(f\"Model {i+1}: y = {w:.4f}x + {b:.4f}, Training data: X={X_train.flatten()}, y={y_train}\")\n",
    "        print(f\"Prediction: X={X_test[0][0]}, Predicted value={predictions[i]:.4f}, Actual value={y[i]}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    return mse, predictions\n",
    "\n",
    "mse, predictions = leave_one_out_cv(X, y)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of leave-one-out cross-validation: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Mean Absolute Error): 1.833\n",
      "MSE (Mean Squared Error): 4.667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "actual_demand = np.array([42, 45, 49, 55, 57, 60, 62, 58, 54, 50, 44, 40])\n",
    "forecast_demand = np.array([44, 46, 48, 50, 55, 60, 64, 60, 53, 48, 42, 38])\n",
    "\n",
    "# Calculate MAE (Mean Absolute Error)\n",
    "mae = np.mean(np.abs(actual_demand - forecast_demand))\n",
    "\n",
    "# Calculate MSE (Mean Squared Error)\n",
    "mse = np.mean((actual_demand - forecast_demand)**2)\n",
    "\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.3f}\")\n",
    "print(f\"MSE (Mean Squared Error): {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class A:\n",
      "  Precision: 0.533\n",
      "  Recall: 0.571\n",
      "Class B:\n",
      "  Precision: 0.739\n",
      "  Recall: 0.531\n",
      "Class C:\n",
      "  Precision: 0.286\n",
      "  Recall: 0.667\n",
      "\n",
      "Overall metrics:\n",
      "  Macro-average Precision: 0.5194\n",
      "  Macro-average Recall: 0.5898\n",
      "  Weighted-average Precision: 0.6314\n",
      "  Weighted-average Recall: 0.5577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the confusion matrix\n",
    "confusion_matrix = np.array([\n",
    "    [40, 20, 10],  # Actual A\n",
    "    [35, 85, 40],  # Actual B\n",
    "    [0, 10, 20]    # Actual C\n",
    "])\n",
    "\n",
    "# Calculate class-wise metrics\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    class_counts = np.sum(confusion_matrix, axis=1)  \n",
    "    total_samples = np.sum(class_counts)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        # Precision: TP / (TP + FP) = diagonal value / column sum\n",
    "        precision[i] = confusion_matrix[i, i] / np.sum(confusion_matrix[:, i])\n",
    "        \n",
    "        # Recall: TP / (TP + FN) = diagonal value / row sum\n",
    "        recall[i] = confusion_matrix[i, i] / np.sum(confusion_matrix[i, :])\n",
    "    \n",
    "    # Macro average (simple average of all classes)\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    \n",
    "    # Weighted average (weighted by class counts)\n",
    "    weighted_precision = np.sum(precision * class_counts) / total_samples\n",
    "    weighted_recall = np.sum(recall * class_counts) / total_samples\n",
    "    \n",
    "    return precision, recall, macro_precision, macro_recall, weighted_precision, weighted_recall, class_counts\n",
    "\n",
    "# Get results\n",
    "precision, recall, macro_precision, macro_recall, weighted_precision, weighted_recall, class_counts = calculate_metrics(confusion_matrix)\n",
    "\n",
    "# Output results\n",
    "class_names = ['A', 'B', 'C']\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"Class {name}:\")\n",
    "    print(f\"  Precision: {precision[i]:.3f}\")\n",
    "    print(f\"  Recall: {recall[i]:.3f}\")\n",
    "\n",
    "print(\"\\nOverall metrics:\")\n",
    "print(f\"  Macro-average Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Macro-average Recall: {macro_recall:.4f}\")\n",
    "print(f\"  Weighted-average Precision: {weighted_precision:.4f}\")\n",
    "print(f\"  Weighted-average Recall: {weighted_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
